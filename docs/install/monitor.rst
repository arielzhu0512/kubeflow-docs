.. _monitor:

======================================
Monitoring with Prometheus and Grafana 
======================================

This document will explain how to provides cluster monitoring services by implementing the open source Prometheus and Grafana projects.

Prerequisites
=============

- A bootstrap machine with the following installed: kubectl and helm.

- A Tanzu Kubernetes Grid cluster running on vSphere, with the package repository installed. For information about deploying, see `vSphere with Tanzu Configuration and Management <https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-152BE7D2-E227-4DAA-B527-557B564D9718.html>`__.

- Connect to the cluster from your client host. See `Connect to a Tanzu Kubernetes Cluster as a vCenter Single Sign-On User <https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-AA3CA6DC-D4EE-47C3-94D9-53D680E43B60.html>`__

- We will be utilizing and monitoring GPU resources on vSphere platform, setup vGPU TKG with document `Deploy AI/ML Workloads on Tanzu Kubernetes Clusters <https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-2B4CAE86-BAF4-4411-ABB1-D5F2E9EF0A3D.html>`__.

Monitor with Prometheus and Grafana
===================================

Deploy Prometheus Operator and Grafana
--------------------------------------

Set up Prometheus
"""""""""""""""""

Deploying a Prometheus stack may seem like a complex undertaking, but leveraging the Helm package manager, 
along with the Prometheus Operator and kube-prometheus projects, can simplify the process. The Operator 
leverages predefined configurations and dashboards for Prometheus and Grafana, while the Helm 
prometheus-operator chart facilitates the installation of Prometheus Operator and all other necessary 
components, resulting in a comprehensive cluster monitoring solution.

First, add the helm repo:

.. code-block:: shell

    $ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    
Now, search for the available prometheus charts:

.. code-block:: shell

    $ helm repo update
    $ helm search repo kube-prometheus

Once you've located which the version of the chart to use, inspect the chart so we can modify the settings:

.. code-block:: shell

    $ helm inspect values prometheus-community/kube-prometheus-stack > /tmp/kube-prometheus-stack.values

Next, we'll need to edit the values file to change the port at which the Prometheus server service is available. 
In the prometheus instance section of the chart, change the service type from ClusterIP to LoadBalancer. 
This will allow the Prometheus server to be accessible at external ip address.

.. code-block:: shell

    From:
    ## Configuration for Prometheus service
    ##
    service:
      ## Service type
      ##
      type: ClusterIP

    To:
    ## Configuration for Prometheus service
    ##
    service:
      ## Service type
      ##
      type: LoadBalancer

Modify the prometheusSpec.serviceMonitorSelectorNilUsesHelmValues settings to false below:

.. code-block:: shell

    ## If true, a nil or {} value for prometheus.prometheusSpec.serviceMonitorSelector will cause the
    ## prometheus resource to be created with selectors based on values in the helm deployment,
    ## which will also match the servicemonitors created
    ##
    serviceMonitorSelectorNilUsesHelmValues: false

You can change the grafana's default login password as follows:

.. code-block:: shell

    ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
    ##
    grafana:
      adminPassword: Grafana1!


Add the following configMap to the section on additionalScrapeConfigs in the Helm chart.

.. code-block:: shell

    # AdditionalScrapeConfigs allows specifying additional Prometheus scrape configurations. Scrape configurations
    # are appended to the configurations generated by the Prometheus Operator. Job configurations must have the form
    # as specified in the official Prometheus documentation:
    # https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config. As scrape configs are
    # appended, the user is responsible to make sure it is valid. Note that using this feature may expose the possibility
    # to break upgrades of Prometheus. It is advised to review Prometheus release notes to ensure that no incompatible
    # scrape configs are going to break Prometheus after the upgrade.
    #
    # The scrape configuration example below will find master nodes, provided they have the name .*mst.*, relabel the
    # port to 2379 and allow etcd scraping provided it is running on all Kubernetes master nodes
    #
    additionalScrapeConfigs:
    - job_name: gpu-metrics
      scrape_interval: 1s
      metrics_path: /metrics
      scheme: http
      kubernetes_sd_configs:
      - role: endpoints
        namespaces:
        names:
        - gpu-operator
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_node_name]
        action: replace
        target_label: kubernetes_node

Finally, we can deploy the Prometheus and Grafana pods using the kube-prometheus-stack via Helm:

.. code-block:: shell

    $ helm install --name-template kube-prometheus-stack prometheus-community/kube-prometheus-stack \
    --create-namespace --namespace prometheus \
    --values ./kube-prometheus-stack.values

You should see a console output as below:

.. code-block:: text
    
    Release "kube-prometheus-stack" does not exist. Installing it now.
    NAME: kube-prometheus-stack
    LAST DEPLOYED: Thu Apr 13 11:43:28 2023
    NAMESPACE: prometheus
    STATUS: deployed
    REVISION: 1
    NOTES:
    kube-prometheus-stack has been installed. Check its status by running:
    kubectl --namespace prometheus get pods -l "release=kube-prometheus-stack"

    Visit https://github.com/prometheus-operator/kube-prometheus for instructions on how to create & configure Alertmanager and Prometheus instances using the Operator.

Now you can see the Prometheus and Grafana pods, ensure the pods are up and running and the validator pods have been completed.

.. code-block:: shell

    $ kubectl get pods -n prometheus

    # NAME                                                       READY   STATUS    RESTARTS       AGE
    # alertmanager-kube-prometheus-stack-alertmanager-0          2/2     Running   1 (4h7m ago)   4h7m
    # kube-prometheus-stack-grafana-7f4454cfb6-fszwv             3/3     Running   0              4h8m
    # kube-prometheus-stack-kube-state-metrics-bc98986bc-krs6l   1/1     Running   0              4h8m
    # kube-prometheus-stack-operator-76c7894576-c9chq            1/1     Running   0              4h8m
    # kube-prometheus-stack-prometheus-node-exporter-24g7f       1/1     Running   0              4h8m
    # kube-prometheus-stack-prometheus-node-exporter-629fx       1/1     Running   0              4h8m
    # kube-prometheus-stack-prometheus-node-exporter-wq72m       1/1     Running   0              4h8m
    # prometheus-kube-prometheus-stack-prometheus-0 polish       2/2     Running   0              4h7m

Patch the Grafana Service
"""""""""""""""""""""""""

You can also launch the Grafana tools for visualizing the GPU metrics. By default, Grafana uses a ClusterIP to expose the ports on which the service is accessible. 
This can be changed to a LoadBalancer instead, so the page is accessible from the browser, similar to the Prometheus dashboard.

.. code-block:: shell 
    
    $ cat << EOF | tee grafana-patch.yaml
    spec:
      type: LoadBalancer
    EOF

And now use kubectl patch:

.. code-block:: shell 

    $ kubectl patch svc kube-prometheus-stack-grafana -n prometheus --patch "$(cat grafana-patch.yaml)"

You can verify that the service is now exposed at an externally accessible port:

.. code-block:: shell 

    kubectl get service -n prometheus

    # NAME                                             TYPE           CLUSTER-IP       EXTERNAL-IP     PORT(S)                      AGE
    # alertmanager-operated                            ClusterIP      None             <none>          9093/TCP,9094/TCP,9094/UDP   3h3m
    # kube-prometheus-stack-alertmanager               ClusterIP      198.57.226.217   <none>          9093/TCP                     3h4m
    # kube-prometheus-stack-grafana                    LoadBalancer   198.59.238.246   10.105.150.43   80:31921/TCP                 3h4m
    # kube-prometheus-stack-kube-state-metrics         ClusterIP      198.53.182.234   <none>          8080/TCP                     3h4m
    # kube-prometheus-stack-operator                   ClusterIP      198.56.158.213   <none>          443/TCP                      3h4m
    # kube-prometheus-stack-prometheus                 LoadBalancer   198.53.121.183   10.105.150.41   9090:31405/TCP               3h4m
    # kube-prometheus-stack-prometheus-node-exporter   ClusterIP      198.57.47.214    <none>          9100/TCP                     3h4m
    # prometheus-operated                              ClusterIP      None             <none>          9090/TCP                     3h3m


Deploy NVIDIA GPU Operator
--------------------------

If your cluster have already installed the gpu operator, modify the clusterpolicy to enable the serviceMonitor as follows: 

.. code-block:: shell
    
    $ kubectl edit clusterpolicy cluster-policy

    # From 
    serviceMonitor:
      enabled: false
    
    # To 
    serviceMonitor:
      enabled: true
    
If not, you can also use the following script, which automates the GPU Operator installation instructions.

.. code-block:: shell
    
    #!/bin/bash
    dir=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
    cd "$dir"
    
    # please filil in api key
    NGC_API_KEY=
    
    #1. Create namespace gpu-operator
    kubectl create namespace gpu-operator
    # 2. Prepare an empty file gridd.conf
    touch gridd.conf
    # 3. Create configmap
    # 3.1. Prepare an empty file gridd.conf
    # 3.2 Prepare your NLS client token file client_configuration_token.tok
    [[ -e ./magpipeline/gpu_operator/nvaie/client_configuration_token.tok ]] || git clone git@gitlab.eng.vmware.com:magqe/magpipeline.git
    # 3.3 Create configmap with above two files
    kubectl create configmap licensing-config --from-file=./gridd.conf --from-file=./magpipeline/gpu_operator/nvaie/client_configuration_token.tok -n gpu-operator
    # 4. Create secret
    kubectl create secret docker-registry ngc-secret \
        --docker-server='nvcr.io/nvaie' \
        --docker-username='$oauthtoken' \
        --docker-password=$NGC_API_KEY \
        --docker-email=liy1@vmware.com \
        -n gpu-operator
    # 5. Fetch GPU Operator Helm chart
    helm fetch https://helm.ngc.nvidia.com/nvaie/charts/gpu-operator-3-0-v22.9.1.tgz \
        --username='$oauthtoken' \
        --password=$NGC_API_KEY
    # 6. Install GPU Operator
    helm install gpu-operator gpu-operator-3-0-v22.9.1.tgz -n gpu-operator
    
    cat << EOF > patch.yaml
    spec:
      template:
        spec:
          containers:
          - name: master
            image: harbor-repo.vmware.com/thunder/nfd/node-feature-discovery:v0.10.1
    EOF
    kubectl patch deployment.apps/gpu-operator-node-feature-discovery-master --patch-file=patch.yaml
    cat << EOF > patch.yaml
    spec:
      template:
        spec:
          containers:
          - name: worker
            image: harbor-repo.vmware.com/thunder/nfd/node-feature-discovery:v0.10.1
    EOF
    kubectl patch daemonset.apps/gpu-operator-node-feature-discovery-worker --patch-file=patch.yaml


Monitor the GPU Resource
------------------------

Now, You can observe that the Prometheus server is available at port 9090 on the node's IP address. 
Open your browser to http://<EXTERNAL-IP>:9090. It may take a few minutes for DCGM to 
start publishing the metrics to Prometheus. The metrics availability can be verified by 
typing DCGM_FI_DEV_GPU_UTIL in the event bar to determine if the GPU metrics are visible:

    .. image:: ../_static/prometheus-1.png

Open your browser to http://<EXTERNAL-IP>:80 and view the Grafana login page. Access Grafana home using the 
`admin` username. The password credentials for the login are available in the prometheus.values file 
we edited in the earlier section of the doc:
    
    .. image:: ../_static/grafana-1.png

    .. image:: ../_static/grafana-2.png


Troubleshooting
===============

Delete the Prometheus Chart
---------------------------

This removes all the Kubernetes components associated with the prometheus chart and deletes the release.

.. code-block:: shell 
    
    helm uninstall kube-prometheus-stack -n prometheus 
    kubectl delete ns prometheus
